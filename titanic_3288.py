# -*- coding: utf-8 -*-
"""Titanic 3288.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZvQJg0-YhxvomHaa9KHFGWC8qh5A1Kg

# Titanic Kaggle Competition
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from scipy import stats
import sklearn as sk
import itertools
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from statsmodels.graphics.mosaicplot import mosaic

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import BaggingClassifier

from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

sns.set(style='white', context='notebook', palette='deep')

SEED = 42

train = pd.read_csv("/content/titanic_train.csv")
test = pd.read_csv("/content/titanic_test.csv")
combine = pd.concat([train.drop('Survived', axis = 1),test])

print(train.shape)
train.columns

"""# Deal with NA"""

train.isnull().any()

nas = train.loc[train.isnull().any(axis=1)]
train.isnull().sum()

nas

train = train.dropna(subset = ['Age'])

"""# EDA"""

train.describe()

"""## Histograms of data distributions"""

_ = train.drop(columns=["PassengerId"]).hist(bins=5, figsize=(15, 15))

# Commented out IPython magic to ensure Python compatibility.
surv = train[train['Survived']==1]
nosurv = train[train['Survived']==0]
surv_col = "blue"
nosurv_col = "red"

print("Survived: %i (%.1f percent), Not Survived: %i (%.1f percent), Total: %i"\
#       %(len(surv), 1.*len(surv)/len(train)*100.0,\
        len(nosurv), 1.*len(nosurv)/len(train)*100.0, len(train)))

"""## Graphs of distributions based on survival status"""

# Commented out IPython magic to ensure Python compatibility.
warnings.filterwarnings(action="ignore")
plt.figure(figsize=[12,10])
plt.subplot(331)
sns.distplot(surv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)
sns.distplot(nosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col,
            axlabel='Age')
plt.subplot(332)
sns.barplot(x='Sex', y='Survived', data=train)
plt.subplot(333)
sns.barplot(x='Pclass', y='Survived', data=train)
plt.subplot(334)
sns.barplot(x='Embarked', y='Survived', data=train)
plt.subplot(335)
sns.barplot(x='SibSp', y='Survived', data=train)
plt.subplot(336)
sns.barplot(x='Parch', y='Survived', data=train)
plt.subplot(337)
sns.distplot(np.log10(surv['Fare'].dropna().values+1), kde=False, color=surv_col)
sns.distplot(np.log10(nosurv['Fare'].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')
plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,
                    wspace=0.35)

print("Median age survivors: %.1f, Median age non-survivers: %.1f"\
#       %(np.median(surv['Age'].dropna()), np.median(nosurv['Age'].dropna())))

"""## Pair plots of distributions based on survival status"""

cols = ['Survived','Pclass','Age','SibSp','Parch','Fare']
g = sns.pairplot(data=train.dropna(), vars=cols, size=1.5,
                 hue='Survived', palette=[nosurv_col,surv_col])
g.set(xticklabels=[])

"""# Correlation visualization"""

y_name = 'Survived'
X_names = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']

numerical_features = train.select_dtypes(include=[np.number]).columns
numerical_features = numerical_features.drop('PassengerId', errors='ignore')
numerical_features

plt.figure(figsize=(14,12))
foocorrmat = train[numerical_features].corr()
foo = sns.heatmap(foocorrmat, vmax=0.6, square=True, annot=True)

"""# Model"""

train_X, valid_test_X, train_y, valid_test_y = train_test_split(train[X_names], train[y_name], test_size=0.3, random_state=SEED) # split off training data
valid_X, test_X, valid_y, test_y = train_test_split(valid_test_X, valid_test_y, test_size=0.3, random_state=SEED) # split remainder into validation and test

# summarize
print('Training data: %i rows and %i columns' % (train_X.shape[0], train_X.shape[1] + 1))
print('Validation data: %i rows and %i columns' % (valid_X.shape[0], valid_X.shape[1] + 1))
print('Testing data: %i rows and %i columns' % (test_X.shape[0], test_X.shape[1] + 1))

# housekeeping
del valid_test_X
del valid_test_y

"""## Run first model"""

clf_ext = ExtraTreesClassifier(
    max_features=5,
    bootstrap=True,
    oob_score=True,
    n_estimators=1000,
    max_depth=8,
    min_samples_split=2,
    random_state=SEED
    )
clf_ext = clf_ext.fit(train_X, train_y)
score_ext = cross_val_score(clf_ext, train_X, train_y, cv=5).mean()
print(score_ext)

"""## Find best model parameters using validation data"""

clf_ext = ExtraTreesClassifier(max_features=5,bootstrap=True,oob_score=True)
param_grid = { "criterion" : ["gini", "entropy"],
              "min_samples_leaf" : [1, 3, 5, 7, 10],
              "min_samples_split" : [2, 4, 6, 8, 10, 12],
              "n_estimators": [5, 10, 15, 20, 25, 50, 75, 100],
              "random_state": [SEED]}
gs = GridSearchCV(estimator=clf_ext, param_grid=param_grid, scoring='accuracy', cv=3)
gs = gs.fit(valid_X, valid_y)
print(gs.best_score_)
print(gs.best_params_)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
ns_probs = [0 for _ in range(len(valid_X))]

# predict probabilities
#lr_probs = clf_ext.predict_proba(valid_X)
lr_probs = gs.best_estimator_.predict_proba(valid_X)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(valid_y, ns_probs)
ClassScore0 = roc_auc_score(valid_y, lr_probs)
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Validation Data: ROC AUC=%.3f' % (ClassScore0))

"""## Run model with best parameters"""

clf_ext = ExtraTreesClassifier(
    max_features=5,
    bootstrap=True,
    oob_score=True,
    criterion='entropy',
    min_samples_leaf=3,
    min_samples_split=8,
    n_estimators=20,
    random_state=SEED
    )
clf_ext = clf_ext.fit(train_X, train_y)
score_ext = clf_ext.score(train_X, train_y)
print(score_ext)
print('\nFeature Importance:')
pd.DataFrame(list(zip(train_X.columns, np.transpose(clf_ext.feature_importances_))) \
            ).sort_values(1, ascending=False)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
ns_probs = [0 for _ in range(len(train_y))]

# predict probabilities
lr_probs = clf_ext.predict_proba(train_X)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(train_y, ns_probs)
ClassScore0 = roc_auc_score(train_y, lr_probs)
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Training Data: ROC AUC=%.3f' % (ClassScore0))

importances = clf_ext.feature_importances_
#importances = gs.best_estimator_.feature_importances_

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': valid_X.columns,
    'Importance': importances
})

# Sort features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display feature importances
print(feature_importance_df)

"""## Run model on test data"""

y_pred = clf_ext.predict(test_X)

"""#### Create submission"""

print("Hyperparameters of model:")
for param, value in clf_ext.get_params().items():
    print(f"{param}: {value}")

clf = clf_ext
df2 = test.loc[:,X_names].fillna(method='pad')
surv_pred = clf.predict(df2)

submit = pd.DataFrame({'PassengerId' : test.loc[:,'PassengerId'],
                       'Survived': surv_pred.T})
submit.to_csv("/content/titanic_train.csv", index=False)
submit.to_csv("submit.csv", index=False)

